{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neven-x/Social-Hierarchy-RL/blob/main/RL_Social_Hierarchy_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jmu7fw-JNem8",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-07-20T10:32:49.925784Z",
     "end_time": "2023-07-20T10:32:49.932729Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import functools\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv3D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from pettingzoo.utils.env import ParallelEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jq_Xm1J6ysY8",
    "outputId": "216e14d0-540a-48a8-86ee-f7ecd0cd7efc",
    "ExecuteTime": {
     "start_time": "2023-07-20T10:32:50.167110Z",
     "end_time": "2023-07-20T10:32:50.171251Z"
    }
   },
   "outputs": [],
   "source": [
    "class Hierarchy_Grid(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"Hierarchy Grid\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, grid_size, num_agents, max_iter):\n",
    "        self.timestep = None\n",
    "        self.grid_size = grid_size\n",
    "        self.agents = np.arange(num_agents)\n",
    "        self.agent_positions = None\n",
    "        self.food_positions = None\n",
    "        self.fight_probs = {name: np.zeros(self.num_agents) for name in self.agents}\n",
    "        self.rewards = {name: 0 for name in self.agents}\n",
    "\n",
    "        self.observation_space = spaces.MultiDiscrete([self.grid_size, self.grid_size, self.num_agents + 1])\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        self.food_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.agent_position_maps = {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "\n",
    "        self.agent_positions = {}\n",
    "        self.agent_position_maps = {}\n",
    "        for agent in self.agents:\n",
    "\n",
    "            agent_position = np.random.randint(0, self.grid_size, 2)\n",
    "            self.agent_positions[agent] = agent_position\n",
    "\n",
    "            agent_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "            agent_position_map[agent_position] = 1\n",
    "            self.agent_position_maps[agent] = agent_position_map\n",
    "\n",
    "        self.food_positions = [np.random.randint(0, self.grid_size, 2)]\n",
    "        self.food_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.food_position_map[self.food_positions[0]] = 1\n",
    "\n",
    "        observations = np.stack(self.agent_position_maps.values())\n",
    "        observations = np.stack([observations, self.food_position_map])\n",
    "\n",
    "        observations = {name: observations for name in self.agents}\n",
    "        return observations, {}\n",
    "\n",
    "    def step(self, actions):\n",
    "\n",
    "        self.timestep += 1\n",
    "\n",
    "        for agent in self.agents:\n",
    "            action = actions[agent]\n",
    "            self.move_agent(agent, action)\n",
    "\n",
    "        self.several_on_food_tile()\n",
    "\n",
    "        if self.timestep > self.max_iter:\n",
    "            terminations = {name: True for name in self.agents}\n",
    "        else:\n",
    "            terminations = {name: False for name in self.agents}\n",
    "\n",
    "        # Add check if any agents have 0 food in which case they die\n",
    "\n",
    "        observations = np.stack(self.agent_position_maps.values())\n",
    "        observations = np.stack([observations, self.food_position_map])\n",
    "\n",
    "        observations = {name: observations for name in self.agents}\n",
    "\n",
    "        return observations, self.rewards, terminations, _, _\n",
    "\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        # Move the agent based on the selected action\n",
    "        x, y = self.agent_positions[agent]\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            x -= 1\n",
    "        elif action == 1:  # Down\n",
    "            x += 1\n",
    "        elif action == 2:  # Left\n",
    "            y -= 1\n",
    "        elif action == 3:  # Right\n",
    "            y += 1\n",
    "        elif action == 4:  # Stay\n",
    "            pass\n",
    "\n",
    "        # Check if the new position is within grid boundaries\n",
    "        if 0 <= x < self.grid_size and 0 <= y < self.grid_size:\n",
    "            self.agent_positions[agent] = (x, y)\n",
    "\n",
    "\n",
    "    def conflict(self, agent1, agent2):\n",
    "\n",
    "        def sig(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        # Agents make decision to fight or leave\n",
    "        # 1 == fight, 0 == leave\n",
    "        decision1 = bool(np.random.binomial(1, sig(self.stay_probs[agent1][agent2])))\n",
    "        decision2 = bool(np.random.binomial(1, sig(self.stay_probs[agent2][agent1])))\n",
    "\n",
    "\n",
    "        # Outcome of fight is determined in case both decide to stay\n",
    "        outcome = np.random.binomial(1, 0.5)\n",
    "        if outcome == 0:\n",
    "            outcome = -1\n",
    "\n",
    "        if not (decision1):\n",
    "            self.relocate_agent(agent1)\n",
    "        if not (decision2):\n",
    "            self.relocate_agent(agent2)\n",
    "\n",
    "        reward_dict = {(False, False): (0, 0),\n",
    "                       (True, False): (5, 0),\n",
    "                       (False, True): (0, 5),\n",
    "                       (True, True): (5 * outcome, 5 * np.delete([-1,1], outcome))}\n",
    "\n",
    "        # Allocate rewards based on decisions and fight outcome\n",
    "        reward1, reward2 = reward_dict((decision1, decision2))\n",
    "        self.reward[agent1] += reward1\n",
    "        self.reward[agent2] += reward2\n",
    "\n",
    "        # Update future staying probabilities\n",
    "        lr = 0.01\n",
    "        self.stay_prob[agent1][agent2] += lr * reward1 * (decision1 - sig(self.stay_prob[agent1][agent2]))\n",
    "        self.stay_prob[agent2][agent1] += lr * reward2 * (decision2 - sig(self.stay_prob[agent2][agent1]))\n",
    "\n",
    "    def several_on_food_tile(self):\n",
    "\n",
    "        for food_tile in self.food_positions:\n",
    "            agents_on_tile = [agent for agent, position in self.agent_positions.items() if position == food_tile]\n",
    "\n",
    "        if len(agents_on_tile) > 1:\n",
    "\n",
    "            pairs = zip(agents_on_tile[:-1], agents_on_tile[1:])\n",
    "\n",
    "            for pair in pairs:\n",
    "                \n",
    "                self.conflict(pair[0], pair[1])\n",
    "\n",
    "\n",
    "    def relocate_agent(self, agent):\n",
    "        # Relocate the agent to an adjacent position\n",
    "        agent_position = self.agent_positions[agent]\n",
    "\n",
    "        valid_position = False\n",
    "        step = 1\n",
    "        while not valid_position:\n",
    "\n",
    "            # Generate moves\n",
    "            possible_moves = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]]) * step  # Right, Left, Down, Up\n",
    "\n",
    "            # Check if any of new positions are valid (within gridworld and not already occupied)\n",
    "            for move in possible_moves:\n",
    "                new_position = tuple(map(sum, zip(agent_position, move)))\n",
    "\n",
    "                if 0 <= new_position[0] < self.grid_size and 0 <= new_position[1] < self.grid_size:\n",
    "                    if new_position not in self.agent_positions.values():\n",
    "\n",
    "                        valid_position = True\n",
    "                        self.agent_positions[agent] = new_position\n",
    "                        agent_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "                        agent_position_map[new_position] = 1\n",
    "                        self.agent_positions_maps[agent] = agent_position_map\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    def render(self):\n",
    "\n",
    "        fig = plt.figure(figsize=(5,5), frameon=False)\n",
    "\n",
    "        plt.title(\"Grid World\",size=13)\n",
    "        plt.xticks(np.arange(0,self.greed_size,1))\n",
    "        plt.yticks(np.arange(0,self.greed_size,1))\n",
    "\n",
    "        agent_position_map = sum(self.agent_position_maps.values())\n",
    "\n",
    "        plt.imshow(self.food_position_map, vmax = 2, cmap = 'Greens', alpha=0.4, extent=[0, 10, 0, 10])\n",
    "        plt.imshow(agent_position_map, vmax = 2, cmap = 'Reds', alpha=0.4, extent=[0, 10, 0, 10])\n",
    "\n",
    "        ax = plt.gca();\n",
    "        ax.grid()\n",
    "\n",
    "        plt.show()\n",
    "        plt.figure()\n",
    "\n",
    "gym.register(\n",
    "    id='Hierarchy_Grid',\n",
    "    entry_point=Hierarchy_Grid,\n",
    "    kwargs={'grid_size': 10, 'num_agents': 10, 'max_iter': 200}\n",
    ")\n",
    "\n",
    "env = gym.make('Hierarchy_Grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, action_size):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95 # discount rate\n",
    "        self.epsilon = 1.0 # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "        self.food = 50\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.state_shape))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):# action selection, epsilon-greedy.\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    # --- update, update the weight of the CNN in each step\n",
    "    def update(self,state,action,reward,next_state): # I just copied the code from replay\n",
    "        target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "        target_f[0][action] = target\n",
    "        self.model.fit(state,target_f,epochs = 1, verbose = 0)\n",
    "\n",
    "    # ---- replay, learn from the memeory, generate new experience from past and train the model by new experience\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "        if not done:\n",
    "            target = (reward + self.gamma *\n",
    "                      np.amax(self.model.predict(next_state)[0])) # Q-learning,model predict is output of all values\n",
    "        target_f = self.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=0) # guess, do gradient descent according to target value\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-20T10:35:26.877038Z",
     "end_time": "2023-07-20T10:35:26.881279Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "AI6-NSubtqn5"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dimension value must be integer or None or have an __index__ method, got value 'Discrete(10)' with type '<class 'gym.spaces.discrete.Discrete'>'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m players \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(Np):\n\u001B[0;32m----> 8\u001B[0m     players\u001B[38;5;241m.\u001B[39mappend(\u001B[43mDQNAgent\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43mn_actions\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     10\u001B[0m state \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset() \u001B[38;5;66;03m# initial state, maybe we don't want so reset the fight probs\u001B[39;00m\n\u001B[1;32m     11\u001B[0m Neps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1000\u001B[39m\n",
      "Cell \u001B[0;32mIn[27], line 11\u001B[0m, in \u001B[0;36mDQNAgent.__init__\u001B[0;34m(self, state_shape, action_size)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepsilon_decay \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.995\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlearning_rate \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.001\u001B[39m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_build_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfood \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m50\u001B[39m\n",
      "Cell \u001B[0;32mIn[27], line 16\u001B[0m, in \u001B[0;36mDQNAgent._build_model\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_build_model\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     15\u001B[0m     model \u001B[38;5;241m=\u001B[39m Sequential()\n\u001B[0;32m---> 16\u001B[0m     \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd\u001B[49m\u001B[43m(\u001B[49m\u001B[43mConv2D\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrelu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate_shape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     model\u001B[38;5;241m.\u001B[39madd(Conv2D(\u001B[38;5;241m64\u001B[39m, kernel_size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m), activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     18\u001B[0m     model\u001B[38;5;241m.\u001B[39madd(Flatten())\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:204\u001B[0m, in \u001B[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    202\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m    203\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 204\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_self_setattr_tracking \u001B[38;5;241m=\u001B[39m previous_value  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/tensor_shape.py:217\u001B[0m, in \u001B[0;36mDimension.__init__\u001B[0;34m(self, value)\u001B[0m\n\u001B[1;32m    215\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(value\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__index__\u001B[39m())\n\u001B[1;32m    216\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m:\n\u001B[0;32m--> 217\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    218\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimension value must be integer or None or have \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    219\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man __index__ method, got value \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{0!r}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m with type \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{1!r}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    220\u001B[0m           value, \u001B[38;5;28mtype\u001B[39m(value))) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    221\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    222\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDimension \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m must be >= 0\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value)\n",
      "\u001B[0;31mTypeError\u001B[0m: Dimension value must be integer or None or have an __index__ method, got value 'Discrete(10)' with type '<class 'gym.spaces.discrete.Discrete'>'"
     ]
    }
   ],
   "source": [
    "env = gym.make('Hierarchy_Grid')\n",
    "n_actions = env.action_space.n\n",
    "state_shape = env.observation_space\n",
    "\n",
    "Np = 10\n",
    "players = []\n",
    "for i in range(Np):\n",
    "    players.append(DQNAgent(state_shape, n_actions))\n",
    "\n",
    "state = env.reset()  # initial state, maybe we don't want to reset the fight probs\n",
    "Neps = 1000\n",
    "\n",
    "# ---- model training/visualization loop\n",
    "for eps in range(Neps):\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = []\n",
    "        # get actions of all agents\n",
    "        env.render()\n",
    "        for p in range(Np):\n",
    "            action.append(players[p].act(state))\n",
    "        next_state, reward, temp = env.step(action)  # we need reward to be a list of size Np\n",
    "        done = np.all(temp.values())\n",
    "\n",
    "        for p in range(Np):  # the DQN should also be updated in each step\n",
    "            players[p].remember(state, action[p], reward[p], next_state, done)\n",
    "\n",
    "        if not done:\n",
    "            for p in range(Np):\n",
    "                # train the model in each step\n",
    "                players[p].update(state, action[p], reward[p], next_state)\n",
    "                state = next_state\n",
    "\n",
    "    # train the model by replay, leverages memory\n",
    "    batch_size = 32  # try a larger batch size for better stability and efficiency\n",
    "    for p in range(Np):\n",
    "        players[p].replay(batch_size)\n",
    "\n",
    "    state = env.reset()  # Reset the environment at the start of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "MultiDiscrete([10 10 11])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-20T10:34:45.679227Z",
     "end_time": "2023-07-20T10:34:45.681833Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-20T10:34:53.667669Z",
     "end_time": "2023-07-20T10:34:53.681010Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
