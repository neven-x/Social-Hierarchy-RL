{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neven-x/Social-Hierarchy-RL/blob/main/RL_Social_Hierarchy_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmu7fw-JNem8",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-11-14T18:25:14.028041Z",
     "end_time": "2023-11-14T18:25:25.141664Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import functools\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers.legacy import Adam\n",
    "\n",
    "import torch\n",
    "\n",
    "from pettingzoo.utils.env import ParallelEnv\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def set_device():\n",
    "    \"\"\"\n",
    "    Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "    Args:\n",
    "      None\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if device != \"cuda\":\n",
    "        print(\"WARNING: For this notebook to perform best, \"\n",
    "              \"if possible, in the menu under `Runtime` -> \"\n",
    "              \"`Change runtime type.`  select `GPU` \")\n",
    "    else:\n",
    "        print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "    return device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-14T18:25:25.141465Z",
     "end_time": "2023-11-14T18:25:25.147991Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jq_Xm1J6ysY8",
    "outputId": "216e14d0-540a-48a8-86ee-f7ecd0cd7efc",
    "ExecuteTime": {
     "start_time": "2023-07-24T11:39:01.132200Z",
     "end_time": "2023-07-24T11:39:01.218621Z"
    }
   },
   "outputs": [],
   "source": [
    "class Hierarchy_Grid(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"Hierarchy Grid\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, grid_size, num_agents, num_food, max_iter):\n",
    "        self.timestep = None\n",
    "        self.grid_size = grid_size\n",
    "        self.possible_agents = np.arange(num_agents)\n",
    "        self.num_food = num_food\n",
    "        self.agents = np.arange(num_agents)\n",
    "        self.max_iter = max_iter\n",
    "        self.agent_positions = None\n",
    "        self.food_positions = None\n",
    "        self.fight_probs = {name: np.zeros(self.num_agents) for name in self.agents}\n",
    "        self.rewards = {name: 0 for name in self.agents}\n",
    "\n",
    "        self.observation_space = spaces.MultiDiscrete([self.grid_size, self.grid_size, self.num_agents + 1])\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        self.food_positions = []\n",
    "        self.food_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.agent_position_maps = {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "\n",
    "        self.num_fights = 0\n",
    "\n",
    "        self.agent_positions = {}\n",
    "        self.agent_position_maps = {}\n",
    "        for agent in self.agents:\n",
    "\n",
    "            agent_position = np.random.randint(0, self.grid_size, 2)\n",
    "            self.agent_positions[agent] = agent_position\n",
    "\n",
    "            agent_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "            agent_position_map[agent_position[0], agent_position[1]] = 1\n",
    "            self.agent_position_maps[agent] = agent_position_map\n",
    "\n",
    "        self.food_positions = []\n",
    "        self.food_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "        for n in range(self.num_food):\n",
    "            self.generate_food()\n",
    "\n",
    "        observations = np.stack(self.agent_position_maps.values())\n",
    "        observations = np.concatenate([observations, np.expand_dims(self.food_position_map, axis=-1)], axis=-1)\n",
    "        observations = np.expand_dims(observations, axis=0)\n",
    "\n",
    "        observations = {name: observations for name in self.agents}\n",
    "        return observations, {}\n",
    "\n",
    "    def step(self, actions):\n",
    "\n",
    "        self.timestep += 1\n",
    "        self.rewards = {name: 0 for name in self.agents}\n",
    "\n",
    "        for agent in self.agents:\n",
    "            action = actions[agent]\n",
    "            self.move_agent(agent, action)\n",
    "\n",
    "        self.reward_from_food()\n",
    "\n",
    "        #self.several_on_food_tile()\n",
    "        self.depl_food()\n",
    "\n",
    "        if self.timestep > self.max_iter:\n",
    "            terminations = {name: True for name in self.agents}\n",
    "        else:\n",
    "            terminations = {name: False for name in self.agents}\n",
    "\n",
    "        observations = np.stack(self.agent_position_maps.values())\n",
    "        observations = np.concatenate([observations, np.expand_dims(self.food_position_map, axis=-1)], axis=-1)\n",
    "        observations = np.expand_dims(observations, axis=0)\n",
    "\n",
    "        observations = {name: observations for name in self.agents}\n",
    "\n",
    "        return observations, self.rewards, terminations, _, {}\n",
    "\n",
    "    def generate_food(self):\n",
    "        new_tile = np.random.randint(0, self.grid_size, 2)\n",
    "        self.food_positions.append(new_tile)\n",
    "        self.food_position_map[new_tile[0], new_tile[1]] += 1\n",
    "\n",
    "    def depl_food(self):\n",
    "        for agent in self.agents:\n",
    "            agent_position = self.agent_positions[agent]\n",
    "            if self.food_position_map[agent_position[0], agent_position[1]] > 0:\n",
    "                self.food_position_map[agent_position[0], agent_position[1]] -= 1/5\n",
    "\n",
    "                if self.food_position_map[agent_position[0], agent_position[1]] <= 0:\n",
    "                    self.food_positions = [tile for tile in self.food_positions if not np.array_equal(tile, agent_position)]\n",
    "\n",
    "                if len(self.food_positions) < self.num_food:\n",
    "                    self.generate_food()\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        # Move the agent based on the selected action\n",
    "        x, y = self.agent_positions[agent]\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            x -= 1\n",
    "        elif action == 1:  # Down\n",
    "            x += 1\n",
    "        elif action == 2:  # Left\n",
    "            y -= 1\n",
    "        elif action == 3:  # Right\n",
    "            y += 1\n",
    "        elif action == 4:  # Stay\n",
    "            pass\n",
    "\n",
    "        # Check if the new position is within grid boundaries\n",
    "        if 0 <= x < self.grid_size and 0 <= y < self.grid_size:\n",
    "            self.agent_positions[agent] = (x, y)\n",
    "\n",
    "            new_map = np.zeros((self.grid_size, self.grid_size))\n",
    "            new_map[x, y] = 1\n",
    "            self.agent_position_maps[agent] = new_map\n",
    "        else:\n",
    "            self.rewards[agent] -= 2\n",
    "\n",
    "    def conflict(self, agent1, agent2):\n",
    "\n",
    "        print(f\"Conflict happened between agents {agent1}, {agent2}\")\n",
    "\n",
    "        def sig(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        # Agents make decision to fight or leave\n",
    "        # 1 == fight, 0 == leave\n",
    "        decision1 = bool(np.random.binomial(1, sig(self.fight_probs[agent1][agent2])))\n",
    "        decision2 = bool(np.random.binomial(1, sig(self.fight_probs[agent2][agent1])))\n",
    "\n",
    "        # Outcome of fight is determined in case both decide to stay\n",
    "        outcome = np.random.binomial(1, 0.5)\n",
    "        if outcome == 0:\n",
    "            outcome = -1\n",
    "\n",
    "        if not (decision1):\n",
    "            self.relocate_agent(agent1)\n",
    "        if not (decision2):\n",
    "            self.relocate_agent(agent2)\n",
    "\n",
    "        reward_dict = {(False, False): (0, 0),\n",
    "                       (True, False): (5, 0),\n",
    "                       (False, True): (0, 5),\n",
    "                       (True, True): (5 * outcome, 5 * np.delete([-1,1], outcome))}\n",
    "\n",
    "        # Allocate rewards based on decisions and fight outcome\n",
    "        reward1, reward2 = reward_dict[(decision1, decision2)]\n",
    "        self.rewards[agent1] += reward1\n",
    "        self.rewards[agent2] += reward2\n",
    "\n",
    "        if decision1 and decision2:\n",
    "            self.num_fights += 1\n",
    "\n",
    "        # Update future staying probabilities\n",
    "        lr = 0.01\n",
    "        self.fight_probs[agent1][agent2] += lr * reward1 * (decision1 - sig(self.fight_probs[agent1][agent2]))\n",
    "        self.fight_probs[agent2][agent1] += lr * reward2 * (decision2 - sig(self.fight_probs[agent2][agent1]))\n",
    "\n",
    "    def several_on_food_tile(self):\n",
    "\n",
    "        for food_tile in self.food_positions:\n",
    "            agents_on_tile = [agent for agent, position in self.agent_positions.items() if (position == food_tile).all()]\n",
    "\n",
    "            if len(agents_on_tile) > 1:\n",
    "\n",
    "                pairs = zip(agents_on_tile[:-1], agents_on_tile[1:])\n",
    "\n",
    "                for pair in pairs:\n",
    "                    self.conflict(pair[0], pair[1])\n",
    "\n",
    "    def reward_from_food(self):\n",
    "\n",
    "        for food_tile in self.food_positions:\n",
    "            agents_on_tile = [agent for agent, position in self.agent_positions.items() if (position == food_tile).all()]\n",
    "\n",
    "            for agent in agents_on_tile:\n",
    "                self.rewards[agent] += 5\n",
    "\n",
    "    def relocate_agent(self, agent):\n",
    "        # Relocate the agent to an adjacent position\n",
    "        agent_position = self.agent_positions[agent]\n",
    "\n",
    "        valid_position = False\n",
    "        step = 1\n",
    "        while not valid_position:\n",
    "\n",
    "            # Generate moves\n",
    "            possible_moves = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]]) * step  # Right, Left, Down, Up\n",
    "\n",
    "            # Check if any of new positions are valid (within gridworld and not already occupied)\n",
    "            for move in possible_moves:\n",
    "                new_position = tuple(map(sum, zip(agent_position, move)))\n",
    "\n",
    "                if 0 <= new_position[0] < self.grid_size and 0 <= new_position[1] < self.grid_size:\n",
    "                    if np.isin((1,2), list(env.agent_positions.values())).any():\n",
    "\n",
    "                        valid_position = True\n",
    "                        self.agent_positions[agent] = new_position\n",
    "                        agent_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "                        agent_position_map[new_position] = 1\n",
    "                        self.agent_position_maps[agent] = agent_position_map\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    def render(self):\n",
    "\n",
    "        fig = plt.figure(figsize=(5,5), frameon=False)\n",
    "\n",
    "        plt.title(\"Grid World\",size=13)\n",
    "        plt.xticks(np.arange(0,self.grid_size,1))\n",
    "        plt.yticks(np.arange(0,self.grid_size,1))\n",
    "\n",
    "        agent_position_map = sum(self.agent_position_maps.values())\n",
    "\n",
    "        plt.imshow(self.food_position_map, vmax = 2, cmap = 'Greens', alpha=0.4, extent=[0, 10, 0, 10])\n",
    "        plt.imshow(agent_position_map, vmax = 2, cmap = 'Reds', alpha=0.4, extent=[0, 10, 0, 10])\n",
    "\n",
    "        ax = plt.gca();\n",
    "        ax.grid()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "gym.register(\n",
    "    id='Hierarchy_Grid',\n",
    "    entry_point=Hierarchy_Grid,\n",
    "    kwargs={'grid_size': 10, 'num_agents': 10, 'num_food': 20,'max_iter': 1000}\n",
    ")\n",
    "\n",
    "env = gym.make('Hierarchy_Grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, action_size):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 0.05  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.loss = []\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', name='Conv1', input_shape=self.state_shape))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', name='Conv2'))\n",
    "        model.add(MaxPool2D(name='MaxPool'))\n",
    "        model.add(Flatten(name='Flatten'))\n",
    "        model.add(Dense(128, activation='relu', name='Dense'))\n",
    "        model.add(Dense(self.action_size, activation='linear', name='output'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):  # action selection, epsilon-greedy.\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            return np.argmax(act_values[0])\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "        target_f = self.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "\n",
    "        history = self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        loss = history.history['loss'][0]\n",
    "        self.loss.append(loss)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-24T11:39:01.900731Z",
     "end_time": "2023-07-24T11:39:01.909532Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI6-NSubtqn5",
    "ExecuteTime": {
     "start_time": "2023-07-24T11:05:16.296927Z",
     "end_time": "2023-07-24T11:21:10.109586Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "env = gym.make('Hierarchy_Grid')\n",
    "n_actions = env.action_space.n\n",
    "state_shape = env.observation_space\n",
    "max_time = 100\n",
    "Np = env.num_agents\n",
    "Neps = 10\n",
    "\n",
    "# Results variables\n",
    "returns = np.zeros((Neps, env.num_agents))\n",
    "num_fights = []\n",
    "\n",
    "# Create agents\n",
    "players = []\n",
    "for i in range(Np):\n",
    "    players.append(DQNAgent((10, 10, 11), n_actions))\n",
    "\n",
    "# ---- model training/visualization loop\n",
    "DEVICE = set_device()\n",
    "for eps in tqdm(range(Neps)):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    timestep = 0\n",
    "\n",
    "    ep_returns = np.zeros(env.num_agents)\n",
    "    while timestep < max_time:\n",
    "        print(timestep)\n",
    "        action = []\n",
    "        # get actions of all agents\n",
    "        for p in range(Np):\n",
    "            action.append(players[p].act(state[p]))\n",
    "\n",
    "        next_state, reward, terminations, _, _ = env.step(action)  # we need reward to be a list of size Np\n",
    "\n",
    "        '''for agent in range(Np):  # the DQN should also be updated in each step\n",
    "            players[p].remember(state[agent], action[agent], reward[agent], next_state, done)'''\n",
    "\n",
    "        # Update returns\n",
    "        for p in range(Np):\n",
    "            ep_returns[p] += reward[p]\n",
    "\n",
    "            # train the model in each step\n",
    "            players[p].update(state[p], action[p], reward[p], next_state[p])\n",
    "            state = next_state\n",
    "\n",
    "        timestep += 1\n",
    "\n",
    "    returns[eps] = ep_returns\n",
    "    num_fights.append(env.num_fights)\n",
    "\n",
    "    # Save model weights\n",
    "    for n, model in enumerate(players):\n",
    "        model.save(f'Weights - Agent {n}')\n",
    "\n",
    "    # train the model by replay, leverages memory\n",
    "    '''batch_size = 32  # try a larger batch size for better stability and efficiency\n",
    "    for p in range(Np):\n",
    "        players[p].replay(batch_size)'''\n",
    "\n",
    "losses = {name: agent.loss for name, agent in enumerate(players)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_loss = []\n",
    "\n",
    "for loss in losses.values():\n",
    "    loss = list(loss)\n",
    "\n",
    "    mean_loss.append(loss)\n",
    "\n",
    "mean_loss = np.stack(mean_loss)\n",
    "mean_loss = np.median(mean_loss, axis=0)\n",
    "\n",
    "plt.plot(mean_loss, label = 'Loss (MSE')\n",
    "\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Mean Squared Loss (Median across agents)')\n",
    "\n",
    "plt.ylim(0, 0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-24T11:21:35.734884Z",
     "end_time": "2023-07-24T11:21:35.921006Z"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
