{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/neven-x/Social-Hierarchy-RL/blob/main/RL_Social_Hierarchy_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "jmu7fw-JNem8",
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-07-21T09:23:03.789324Z",
     "end_time": "2023-07-21T09:23:03.804438Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import functools\n",
    "import random\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv3D\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from pettingzoo.utils.env import ParallelEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jq_Xm1J6ysY8",
    "outputId": "216e14d0-540a-48a8-86ee-f7ecd0cd7efc",
    "ExecuteTime": {
     "start_time": "2023-07-21T09:50:27.683333Z",
     "end_time": "2023-07-21T09:50:27.683882Z"
    }
   },
   "outputs": [],
   "source": [
    "class Hierarchy_Grid(ParallelEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"Hierarchy Grid\",\n",
    "    }\n",
    "\n",
    "    def __init__(self, grid_size, num_agents, max_iter):\n",
    "        self.timestep = None\n",
    "        self.grid_size = grid_size\n",
    "        self.agents = np.arange(num_agents)\n",
    "        self.max_iter = max_iter\n",
    "        self.agent_positions = None\n",
    "        self.food_positions = None\n",
    "        self.fight_probs = {name: np.zeros(self.num_agents) for name in self.agents}\n",
    "        self.rewards = {name: 0 for name in self.agents}\n",
    "\n",
    "        self.observation_space = spaces.MultiDiscrete([self.grid_size, self.grid_size, self.num_agents + 1])\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "        self.food_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.agent_position_maps = {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "\n",
    "        self.agent_positions = {}\n",
    "        self.agent_position_maps = {}\n",
    "        for agent in self.agents:\n",
    "\n",
    "            agent_position = np.random.randint(0, self.grid_size, 2)\n",
    "            self.agent_positions[agent] = agent_position\n",
    "\n",
    "            agent_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "            agent_position_map[agent_position[0], agent_position[1]] = 1\n",
    "            self.agent_position_maps[agent] = agent_position_map\n",
    "\n",
    "        self.food_positions = []\n",
    "        self.food_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "        for n in range(6):\n",
    "\n",
    "            new_tile = np.random.randint(0, self.grid_size, 2)\n",
    "            self.food_positions.append(new_tile)\n",
    "            self.food_position_map[new_tile[0], new_tile[1]] += 1\n",
    "\n",
    "        observations = np.stack(self.agent_position_maps.values())\n",
    "        observations = np.concatenate([observations, np.expand_dims(self.food_position_map, axis=-1)], axis=-1)\n",
    "\n",
    "        observations = {name: observations for name in self.agents}\n",
    "        return observations, {}\n",
    "\n",
    "    def step(self, actions):\n",
    "\n",
    "        self.timestep += 1\n",
    "\n",
    "        for agent in self.agents:\n",
    "            action = actions[agent]\n",
    "            self.move_agent(agent, action)\n",
    "\n",
    "        self.several_on_food_tile()\n",
    "\n",
    "        if self.timestep > self.max_iter:\n",
    "            terminations = {name: True for name in self.agents}\n",
    "        else:\n",
    "            terminations = {name: False for name in self.agents}\n",
    "\n",
    "        # Add check if any agents have 0 food in which case they die\n",
    "\n",
    "        observations = np.stack(self.agent_position_maps.values())\n",
    "        observations = np.concatenate([observations, np.expand_dims(self.food_position_map, axis=-1)], axis=-1)\n",
    "\n",
    "        observations = {name: observations for name in self.agents}\n",
    "\n",
    "        return observations, self.rewards, terminations, _, {}\n",
    "\n",
    "\n",
    "    def move_agent(self, agent, action):\n",
    "        # Move the agent based on the selected action\n",
    "        x, y = self.agent_positions[agent]\n",
    "\n",
    "        if action == 0:  # Up\n",
    "            x -= 1\n",
    "        elif action == 1:  # Down\n",
    "            x += 1\n",
    "        elif action == 2:  # Left\n",
    "            y -= 1\n",
    "        elif action == 3:  # Right\n",
    "            y += 1\n",
    "        elif action == 4:  # Stay\n",
    "            pass\n",
    "\n",
    "        # Check if the new position is within grid boundaries\n",
    "        if 0 <= x < self.grid_size and 0 <= y < self.grid_size:\n",
    "            self.agent_positions[agent] = (x, y)\n",
    "\n",
    "\n",
    "    def conflict(self, agent1, agent2):\n",
    "\n",
    "        def sig(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        # Agents make decision to fight or leave\n",
    "        # 1 == fight, 0 == leave\n",
    "        decision1 = bool(np.random.binomial(1, sig(self.fight_probs[agent1][agent2])))\n",
    "        decision2 = bool(np.random.binomial(1, sig(self.fight_probs[agent2][agent1])))\n",
    "\n",
    "\n",
    "        # Outcome of fight is determined in case both decide to stay\n",
    "        outcome = np.random.binomial(1, 0.5)\n",
    "        if outcome == 0:\n",
    "            outcome = -1\n",
    "\n",
    "        if not (decision1):\n",
    "            self.relocate_agent(agent1)\n",
    "        if not (decision2):\n",
    "            self.relocate_agent(agent2)\n",
    "\n",
    "        reward_dict = {(False, False): (0, 0),\n",
    "                       (True, False): (5, 0),\n",
    "                       (False, True): (0, 5),\n",
    "                       (True, True): (5 * outcome, 5 * np.delete([-1,1], outcome))}\n",
    "\n",
    "        # Allocate rewards based on decisions and fight outcome\n",
    "        reward1, reward2 = reward_dict[(decision1, decision2)]\n",
    "        self.rewards[agent1] += reward1\n",
    "        self.rewards[agent2] += reward2\n",
    "\n",
    "        # Update future staying probabilities\n",
    "        lr = 0.01\n",
    "        self.fight_probs[agent1][agent2] += lr * reward1 * (decision1 - sig(self.fight_probs[agent1][agent2]))\n",
    "        self.fight_probs[agent2][agent1] += lr * reward2 * (decision2 - sig(self.fight_probs[agent2][agent1]))\n",
    "\n",
    "    def several_on_food_tile(self):\n",
    "\n",
    "        for food_tile in self.food_positions:\n",
    "            agents_on_tile = [agent for agent, position in self.agent_positions.items() if (position == food_tile).all()]\n",
    "\n",
    "        if len(agents_on_tile) > 1:\n",
    "\n",
    "            pairs = zip(agents_on_tile[:-1], agents_on_tile[1:])\n",
    "\n",
    "            for pair in pairs:\n",
    "                \n",
    "                self.conflict(pair[0], pair[1])\n",
    "\n",
    "\n",
    "    def relocate_agent(self, agent):\n",
    "        # Relocate the agent to an adjacent position\n",
    "        agent_position = self.agent_positions[agent]\n",
    "\n",
    "        valid_position = False\n",
    "        step = 1\n",
    "        while not valid_position:\n",
    "\n",
    "            # Generate moves\n",
    "            possible_moves = np.array([[0, 1], [0, -1], [1, 0], [-1, 0]]) * step  # Right, Left, Down, Up\n",
    "\n",
    "            # Check if any of new positions are valid (within gridworld and not already occupied)\n",
    "            for move in possible_moves:\n",
    "                new_position = tuple(map(sum, zip(agent_position, move)))\n",
    "\n",
    "                if 0 <= new_position[0] < self.grid_size and 0 <= new_position[1] < self.grid_size:\n",
    "                    if np.isin((1,2), list(env.agent_positions.values())).any():\n",
    "\n",
    "                        valid_position = True\n",
    "                        self.agent_positions[agent] = new_position\n",
    "                        agent_position_map = np.zeros((self.grid_size, self.grid_size))\n",
    "                        agent_position_map[new_position] = 1\n",
    "                        self.agent_position_maps[agent] = agent_position_map\n",
    "\n",
    "            step += 1\n",
    "\n",
    "    def render(self):\n",
    "\n",
    "        fig = plt.figure(figsize=(5,5), frameon=False)\n",
    "\n",
    "        plt.title(\"Grid World\",size=13)\n",
    "        plt.xticks(np.arange(0,self.grid_size,1))\n",
    "        plt.yticks(np.arange(0,self.grid_size,1))\n",
    "\n",
    "        agent_position_map = sum(self.agent_position_maps.values())\n",
    "\n",
    "        plt.imshow(self.food_position_map, vmax = 2, cmap = 'Greens', alpha=0.4, extent=[0, 10, 0, 10])\n",
    "        plt.imshow(agent_position_map, vmax = 2, cmap = 'Reds', alpha=0.4, extent=[0, 10, 0, 10])\n",
    "\n",
    "        ax = plt.gca();\n",
    "        ax.grid()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "gym.register(\n",
    "    id='Hierarchy_Grid',\n",
    "    entry_point=Hierarchy_Grid,\n",
    "    kwargs={'grid_size': 10, 'num_agents': 10, 'max_iter': 200}\n",
    ")\n",
    "\n",
    "env = gym.make('Hierarchy_Grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, action_size):\n",
    "        self.state_shape = state_shape\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=self.state_shape))\n",
    "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):  # action selection, epsilon-greedy.\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "        target_f = self.model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-21T09:50:27.863533Z",
     "end_time": "2023-07-21T09:50:27.865550Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "AI6-NSubtqn5",
    "ExecuteTime": {
     "start_time": "2023-07-21T09:50:28.125537Z",
     "end_time": "2023-07-21T09:50:28.548008Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Hierarchy_Grid')\n",
    "n_actions = env.action_space.n\n",
    "state_shape = env.observation_space\n",
    "\n",
    "Np = 10\n",
    "players = []\n",
    "for i in range(Np):\n",
    "    players.append(DQNAgent((10, 10, 11), n_actions))\n",
    "\n",
    "# initial state, maybe we don't want to reset the fight probs\n",
    "Neps = 1000\n",
    "\n",
    "# ---- model training/visualization loop\n",
    "for eps in range(Neps):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = []\n",
    "        # get actions of all agents\n",
    "        #env.render()\n",
    "        for p in range(Np):\n",
    "            action.append(players[p].act(state))\n",
    "        next_state, reward, terminations, _, _ = env.step(action)  # we need reward to be a list of size Np\n",
    "        done = np.all(terminations.values())\n",
    "\n",
    "        '''for agent in range(Np):  # the DQN should also be updated in each step\n",
    "            players[p].remember(state[agent], action[agent], reward[agent], next_state, done)'''\n",
    "\n",
    "        if not done:\n",
    "            for p in range(Np):\n",
    "                # train the model in each step\n",
    "                players[p].update(state, action[p], reward[p], next_state)\n",
    "                state = next_state\n",
    "\n",
    "    # train the model by replay, leverages memory\n",
    "    '''batch_size = 32  # try a larger batch size for better stability and efficiency\n",
    "    for p in range(Np):\n",
    "        players[p].replay(batch_size)'''\n",
    "\n",
    "state = env.reset()  # Reset the environment at the start of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 500x500 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAacAAAHDCAYAAACAitXUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAil0lEQVR4nO3df5xdd13n8denc0PKtEloVqiUilIoBQprgFaLljTdUn6I8nNdYFaUrvKjLK4srmJtoXFFiuLWgkZ3FYGqjPwQhQd0+7C2Jg0/CvaHCLS12B9pSVsoS2kmdtp0bvLZP86d5uZmJpkzmcn9xHk9H4953Nxzz7nnPedO7vt+zzn33shMJEmq5LBhB5AkaZDlJEkqx3KSJJVjOUmSyrGcJEnlWE6SpHIsJ0lSOZaTJKkcy0mSVI7lJPWJiE0RsWWO8/5QRGRErF/cVAsvIrZExKY5zruu93u+bnFTSbtZTjqkRcThEfHmiPj7iPhORExFxH0RcXVE/E5EPKVAxjf1ntzfOMNtP9277Z6IiBluvzYidkTEIw9OWqkGy0mHrIg4DrgO2EDzt/x7wBuAdwBfBc4Cro+Ix7W42+cDJyxw1I29y9NnuG0d0AUeDZzYf0NErALWAF/KzAcWOJNUWmfYAaT56I0kLgGeCLwiM/9mhnkOB/47sM9PN46Iw4DlmflAZj600Fkz86aIuIumiAatAz4OvIKmvL7ed9tamtLduPdi7UVEBxjJzB0LcX/SYnLkpEPVLwBPAd47UzEBZOaDmXlBZt41PS0iXtfbjfa8iHhHRNwC7ABe1bt9xmNOEfGTEXFNRDwYEXdHxPuBI1rk3QQcHRFP7bvPRwNPBf4W+DJ7j6zW9S439i3z9Ij4ZET8v97uvpsi4p0RsXwg7/re73liRFwYEVt7v+dz9hUyIs6KiOt7931773iaL2J10PlHp0PVf+xdfmCey/8uzd//nwATwE2zzRgRLwf+CrgT+C3gfmAM+PEW69vYW2YdcGNv2joggCtpRoBviYjI3d9jsw54APhSL8ezgM3ALppdmVuBFwC/ATwnIl6cmbsG1vuRXt7/RTOCvHsfv+cvARcB1wPn0Wyfs4CfavF7SgvCctKh6unARGbe1j8xIkaAowbmvX+GYzaHA8/c37Gc3v29D9gO/Ehmfqs3fQPwhRZ5+487/VHv3+uALZl5e+/MuXcCPwx8pe9408a+3XDvBx4JnJyZ1/WmbYiIPwZeD7waGB9Y773AmZm5cz+/56OAdwM3A6dk5r/2pv8Re+5qlA4Kd+vpULWSZsQz6KnAdwZ+fmmG+f5ojicZPAv4AeDD08UE0CuMC+caNjNvAb4JnNY3eR3NqAma0dFD7N6Vt8fxpt4uwB8HLukrpmm/2bt8xQyrft/+iqnnTGAU2DBdTL3c9wF/OIflpQVlOelQNUFTUINuo3miPRP4H/tY/l/muJ4n9i5vnOG2G+Z4H9M2Ao/pHQd6NPA0mmNR9IryH9h93Gld3zIAx/Uurx+808z8JrCtb55+w/g9pQNmOelQdT2wMiKe0D8xM+/PzMsz83Lg2n0sP9lyffs842+OpotmHbvL58q+268E1vbOHlxHc6zo6t5te70Hao6G8XtKB8xy0qHqk73LX1jk9dzSu3zaDLfNNG1f+o87rQPuGDhmdiXwqN7ta4DPZebUQI493gsFEBHHAqv65pmPhfw9pQNmOelQ9SfAN4Bf6Z1NN5P5jjb6XUdzrOjnIuL7H77j5tTtt7W5o8y8nWa342k0BXTlwCxfBKZoTozY4/1NmfkdmhMwfiIi1gwsd27v8q/b5BnwdzSjrP8aEUdOT+ydKPHmA7hfaV48W0+HpMycjIgXA58F/rp3tttlwLdojkU9hea9SzuBOw5gPTt7p1j/FfAPvTPj7gf+M/Mrv43AfwG+j+Z09v513R8R19KcDDE9b7//RnMq+ZW9swXvpPlEi5fQvFfqY/PIM73u+yLiHJozE78UERcDI72s3wbafMqGdMAcOemQlZk3A8+kedIO4JeBP6Z538/JNO+BenpmDp5e3XY9fwO8lObMv/OAtwNXAT87j7vrL5zBkVP/tAmaUVt/juuAU4ArgDfSfFzTU4H1wEtneI9TK5n5fpoyOgx4F3A28Jc0v690UMXu9/tJklSDIydJUjmWkySpHMtJklSO5SRJKsdykiSVYzlJkso56G/CjYgAjqH5CgJJ0tKzArgr9/FepmF8QsQxNF+SJklauo6l+ZSTGQ2jnLYD3HzTjaxePfidcMMzNdXlik2bOWPdWpYtq/WpTlWzVc0FdbNVzQV1s1XNBbuznb7qMJbFQnyU48KYymTjtl0lt9nE9u380HHHw372ng0t9YoVK1i5cqav4xmOqakuo6OjrFy5styDWTVb1VxQN1vVXFA3W9Vc0JftiHrlNDq1q+Q2mytPiJAklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUTutyiogVEXFRRNweEQ9ExBcj4uTFCCdJWprmM3L6AHAm8FrgGcBlwOUR8biFDCZJWrpalVNEPBJ4JfCrmbk5M2/OzPXAbcDZi5BPkrQEtR05dYAR4MGB6Q8Apy5IIknSktdpM3Nmbo+Iq4B3RMSNwLeB1wA/CvzLTMtExHJged+kFQDdbpepqe68Qi+GbrfJsuW+LYx0ap0nsrO7C4CHrt7Iroghp9mtm9lcdus8jtOmM1XLVjUX1M1WNRfszrRrzans7LR6Ol1Uu7pd2LS55Dab6/N+ZO8JZq4i4onAB4G1wE7gOuAbwLMy82kzzL8eOH9w+vj4OKOjo63WLUk6tE1OTjI2NgawKjMnZpuvdTk9vGDEEcDKzLw7Ij4GHJmZL55hvplGTlvvvGMLq1evnte6F0O32+WKTZs54aTjSo6cbrrmVk5fFXSKjZw2bkvOWLeWTqFXjbD78ayWrWouqJutai6om61qLoCJiQmOPuZY2E85zTt1Zt4P3B8RRwEvAH51lvl2ADumr0fvybXT6bBsWa2NBjDSOYyRzsiwY8yoE8GyQuXUyLKPJdT9O6uaC+pmq5oL6marmGuueVqnjogXAAHcBDwJeG/v3x9qe1+SJM1kPvuvVgEbgH8G/gz4PPD8zJxayGCSpKWr9cgpMz8OfHwRskiSBPjZepKkgiwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUTqtyiohORLwrIm6LiAci4taIeGdEWHKSpAXTaTn/24E3AT8HXA+cBHwI2Aa8b2GjSZKWqrbl9Bzg05l5Se/6loh4DU1JSZK0INqW0+eBN0XEkzPzGxHxw8CpwFtnWyAilgPL+yatAOh2u0xNdVuufvF0u02Wnd1dQ06yt+lM3cwhJ9nTdJ7pbVfJdKZq2armgrrZquaCutmq5gLm/Lwf2eIJLyICeDfN7r2dwAhwbmZesI9l1gPnD04fHx9ndHR0zuuWJB36JicnGRsbA1iVmROzzdd25PQq4GeAMZpjTmuAiyLirsy8eJZlLgAu7Lu+Ath6/DN/iFVHrWy5+sWzs7uLm665lTPWraXTabtZFle32+WKTZs54aTjGOnUOffkUNhmp68KOhHDjvOwbiYbt2XpbVYtW9VcUDdb1VwAExOz9tEe2qZ+L/CezPxo7/rXIuIHgXOAGcspM3cAO6avR++JYqRzGCOdkZarX3ydTodly2o9mNPcZu11IlhWqJwaWXubFc1WNRfUzVYx11zztH0ZPgoMHpTZOY/7kSRpVm0r9TPAuRFxB81uvWcCbwM+uNDBJElLV9ty+kXgN4E/BB4D3AX8H+B/LnAuSdIS1qqcMnM7zWnjb12MMJIkgceKJEkFWU6SpHIsJ0lSOZaTJKkcy0mSVI7lJEkqx3KSJJVjOUmSyrGcJEnlWE6SpHIsJ0lSOZaTJKkcy0mSVI7lJEkqx3KSJJVjOUmSyrGcJEnlWE6SpHIsJ0lSOZaTJKkcy0mSVI7lJEkqx3KSJJVjOUmSyrGcJEnlWE6SpHIsJ0lSOa3KKSK2RETO8LNhsQJKkpaeTsv5TwZG+q4/Hfg74BMLlkiStOS1KqfM/E7/9Yj4NeAW4MqFDCVJWtrmfcwpIh4B/AzwwczMhYskSVrq2u7W6/cy4FHAh/c1U0QsB5b3TVoB8OibbmP1kUccwOoXVjeTG4At921hpFPrPJGd3V0APPbwY+h0DuQhW1jdbpcbuJlutzvsKHuZzrRrzansLLTNdnW7sGkzD129kV0Rw46zh27vNWa1x3M6T7VcUDdb1VwAU1NzyxTzHfRExN8CD2XmT+1nvvXA+YPTx8fHGR0dnde6JUmHpsnJScbGxgBWZebEbPPN6yVlRPwg8DzgFXOY/QLgwr7rK4Ctz10ZrD6yzgilm8nGbckJJx1XcuR00zW3csa6teVGTlds2lwuF9TNNp3r9FVBp+DIaeO2LLvNquWCutmq5gKYmJi1j/Yw39RnAfcAl+xvxszcAeyYvh69/5CdCJYV+88JyUjnMEY6I/ufdQg6nQ7LltX6Q4O6uaButpp//wBZd5sVzQV1s1XMNdc8rYcIEXEYTTldnJn1dmhKkg5589l/9Tzg8cAHFziLJEnAPHbrZeZlQMX9EZKkfyNqHfmXJAnLSZJUkOUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIkldO6nCLicRHxFxHx3YiYjIivRMSzFyOcJGlp6rSZOSKOAr4AbAReBNwDPBG4b8GTSZKWrFblBLwd+GZmntU3bcvCxZEkqX05vQT424j4BHAacCfwh5n5J7MtEBHLgeV9k1YAdDOZymy5+sXT7WXZ2d015CR7m87U7XaHnGRP03mq5YK62R7OVehvf9p0prLbrFguqJutai6Aqam5ZYps8Z8kIh7s/fNC4BPAjwAXAW/MzD+bZZn1wPmD08fHxxkdHZ3zuiVJh77JyUnGxsYAVmXmxGzztS2nh4BrMvPH+qa9Hzg5M58zyzIzjZy23nnHFlavXj3ndS+2brfLFZs2c8a6tXQ6bQeUi6tqtqq5oG62qrlgd7bTVwWdiGHHeVg3k43bsvQ2q5atai6AiYkJjj7mWNhPObVNfTdww8C0G4FXzrZAZu4Adkxfj94ffafTYdmyWhsN6uaCutmq5oK62armAuhEsKxQOTWy9jYrmq1irrnmaXsq+ReAEwamPRm4veX9SJI0q7bl9HvAKRHx6xHxpIgYA94AbFj4aJKkpapVOWXm1cDLgdcAXwfeAbw1Mz+yCNkkSUtU652RmflZ4LOLkEWSJMDP1pMkFWQ5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeW0KqeIWB8ROfDzrcUKJ0lamjrzWOZ64Hl913cuUBZJkoD5lVM3Mx0tSZIWzXyOOR0fEXdFxG0R8dGIOG7BU0mSlrS2I6cvAz8LfAM4GjgP+GJEnJiZ351pgYhYDizvm7QC4KHrPseOI49on3iRdDOby253yEn2Np2pWrbpPA9dvZFdEUNOs6fpx3PLfVsY6dQ572dndxdQ77GE3Zl2rTmVnZ357FRZHLu6Xdi0ufQ2q5atai6Aqam5ZYrs/Seej4g4ArgF+J3MvHCWedYD5w9OHx8fZ3R0dN7rliQdeiYnJxkbGwNYlZkTs813QC+PMvP+iPgacPw+ZrsA6C+uFcDW564MVh9Z5xVtN5ON25Iz1q2lU+hVIzSvfq7YtLlctulcp68KOgVHThu3JSecdFy5kdNN19xa7rGE+n9n1XJB3WxVcwFMTMzaR3s4oNS9XXZPBT432zyZuQPY0bdMs+IIlhV7QoOk0+mwbFmtB3Na1Ww1H0uAZKRzGCOdkWEH2UvVxxLqZquaC+pmq5hrrnnavs/pdyPitIh4QkT8KPBXwErg4vYRJUmaWdtKPRb4S+D7gO8AXwJOyczbFzqYJGnpalVOmfnqxQoiSdK0OkeKJUnqsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5B1ROEXFORGREXLRAeSRJmn85RcTJwBuAry5cHEmS5llOEXEk8BHg9cD3FjSRJGnJ68xzuQ3AJZl5eUSct68ZI2I5sLxv0gqAbiZTmfNc/cLr9rJ0u90hJ9nbdKZq2R7OVehxnDadaWd315CT7Gk6T7XHEg6Bv7NiuaButqq5AKam5pYpsuUTS0S8GjgXODkzH4yITcBXMvOts8y/Hjh/cPr4+Dijo6Ot1i1JOrRNTk4yNjYGsCozJ2abr9XIKSJ+AHgf8PzMfHCOi10AXNh3fQWw9bRTf4zVq1e3Wf2i6na7XLFpM2esW0unM98B5eKYznb6qqATMew4D+tmsnFblt5mJ5x0HCOdOiel7uzu4qZrbi29zaplq5oL6marmgtgYmLWPtpD29TPBh4DXBu7nyRHgLUR8RZgeWbu7F8gM3cAO6avTy/X6XRYtqzWRoO6uQA6ESwrVE6NLL3NRjqHMdIZGXaMvVTeZlWzVc0FdbNVzDXXPG1TXwE8Y2Dah4B/Bn57sJgkSZqPVuWUmduBr/dPi4j7ge9m5tdnXkqSpHbq7IyXJKnngHdGZua6BcghSdLDHDlJksqxnCRJ5VhOkqRyLCdJUjmWkySpHMtJklSO5SRJKsdykiSVYzlJksqxnCRJ5VhOkqRyLCdJUjmWkySpHMtJklSO5SRJKsdykiSVYzlJksqxnCRJ5VhOkqRyLCdJUjmWkySpHMtJklSO5SRJKsdykiSVYzlJksqxnCRJ5VhOkqRyWpVTRJwdEV+NiInez1UR8aLFCidJWprajpy2Ar8GnNT7+Xvg0xFx4kIHkyQtXZ02M2fmZwYmnRsRZwOnANcvWCpJ0pLWqpz6RcQI8NPAEcBVC5ZIkrTktS6niHgGTRkdDvwr8PLMvGEf8y8HlvdNWgHQ7XaZmuq2Xf2i6Xa7e1xWMp1p15pT2dmZ9+uJBber24VNm0tvs8cefgydQtus2+1yAzeX3mbVslXNBXWzVc0FzPl5PzKz1R1HxCOAxwOPAl4J/AJw2mwFFRHrgfMHp4+PjzM6Otpq3ZKkQ9vk5CRjY2MAqzJzYrb5WpfTXncQcTlwS2a+cZbbZxo5bb3zji2sXr36gNa9kLrdLlds2swZ69aWeqUNdbNVzQV1s1XNBXWzVc0FdbNVzQUwMTHB0cccC/spp4VIHexZPnvIzB3AjodnjmhW3OmwbFmtjQZ1c0HdbFVzQd1sVXNB3WxVc0HdbBVzzTVPq9QR8W7gUuCbNCOgVwPrgBe2iydJ0uzaVurRwJ8DjwW2AV8FXpiZf7fQwSRJS1fb9zn9/GIFkSRpmp+tJ0kqx3KSJJVjOUmSyrGcJEnlWE6SpHIsJ0lSOZaTJKkcy0mSVI7lJEkqx3KSJJVjOUmSyrGcJEnlWE6SpHIsJ0lSOZaTJKkcy0mSVI7lJEkqx3KSJJVjOUmSyrGcJEnlWE6SpHIsJ0lSOZaTJKkcy0mSVI7lJEkqx3KSJJVjOUmSyrGcJEnltCqniDgnIq6OiO0RcU9EfCoiTliscJKkpantyOk0YANwCnAm0AEui4gjFjqYJGnp6rSZOTNf2H89Is4C7gGeDWxewFySpCWsVTnNYFXv8t7ZZoiI5cDyvkkrALrdLlNT3QNc/cLpdrt7XFZSNVvVXFA3W9VcUDdb1VxQN1vVXMCcn/cjM+e1gogI4NPAUZn53H3Mtx44f3D6+Pg4o6Oj81q3JOnQNDk5ydjYGMCqzJyYbb4DKacNwIuBUzNz6z7mm2nktPXOO7awevXqea17MXS7Xa7YtJkz1q2l0znQAeXCqpqtai6om61qLqibrWouqJutai6AiYkJjj7mWNhPOc0rdUT8PvASYO2+igkgM3cAO/qWbVbc6bBsWa2NBnVzQd1sVXNB3WxVc0HdbFVzQd1sFXPNNU+r1L1deb8PvBxYl5m3tY8mSdK+ta3UDcAY8FJge0R8f2/6tsx8YEGTSZKWrLbvczqb5gy9TcDdfT+vWthYkqSlrO37nGKxgkiSNM3P1pMklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeVYTpKkciwnSVI5lpMkqRzLSZJUjuUkSSrHcpIklWM5SZLKsZwkSeW0LqeIWBsRn4mIuyIiI+Jli5BLkrSEzWfkdATwT8BbFjiLJEkAdNoukJmXApcCRMSCB5IkyWNOkqRyWo+c2oqI5cDyvkkrALrdLlNT3cVe/Zx1u909LiuZzvTQ1RvZVWi02s1sLgtvs2rZquaCutmq5oK62armAub8vB/Ze4KZj4hI4OWZ+al9zLMeOH9w+vj4OKOjo/NetyTp0DM5OcnY2BjAqsycmG2+g1FOM42ctt55xxZWr14973UvtG63yxWbNnPGurV0Oos+oGxlOtvpq4JOsZHTxm1ZeptVy1Y1F9TNVjUX1M1WNRfAxMQERx9zLOynnBY9dWbuAHZMX58+iaLT6bBsWa2NBnVzAXQiWFaonBpZe5sVzVY1F9TNVjUX1M1WMddc87ROHRFHAk/qm/SEiFgD3JuZd7S9P0mSBs2nUk8CNvZdv7B3eTHwugMNJEnSfN7ntAmotm9JkvRviO9zkiSVYzlJksqxnCRJ5VhOkqRyLCdJUjmWkySpHMtJklSO5SRJKsdykiSVYzlJksqxnCRJ5VhOkqRyLCdJUjmWkySpHMtJklSO5SRJKsdykiSVYzlJksqxnCRJ5VhOkqRyLCdJUjmWkySpHMtJklSO5SRJKsdykiSVYzlJksqxnCRJ5VhOkqRy5lVOEfHmiLgtIh6MiGsj4rkLHUyStHS1LqeIeBVwEfBbwDOBzwGXRsTjFzaaJGmpms/I6W3An2bmBzLzxsx8K/BN4OwFTSZJWrI6bWaOiEcAzwbeM3DTZcCPzbLMcmB536QVAN/73vfarHrRdbtdJicnuffee+l0Wm2WRfdwtk7QiRh2nId1M5mczNrbrFi2qrmgbraquaButqq5ALZv3z6n+dqm/j5gBPj2wPRvA98/yzLnAOcPTnzKic9ouWpJ0r8hK4CJ2W6cb6XmwPWYYdq0C4ALBwJtBY4F5lahB0fVXFA3W9VcUDdb1VxQN1vVXFA3W9Vc01YAd+1rhrbl9P+Anew9SnoMe4+mAMjMHcCO6euxe7fU9syctTUPtqq5oG62qrmgbraquaButqq5oG62qrn67DdTqxMiMvMh4FrgzIGbzgS+2Oa+JEmazXx2610I/HlEXANcBbwBeDzwvxcymCRp6WpdTpn5sYj4d8A7gccCXwd+IjNvn+Nd7AB+g75dfUVUzQV1s1XNBXWzVc0FdbNVzQV1s1XNNWeROdt5DJIkDYefrSdJKsdykiSVYzlJksqxnCRJ5RzUcqr4VRsRsTYiPhMRd0VERsTLhp0JICLOiYirI2J7RNwTEZ+KiBOGnQsgIs6OiK9GxETv56qIeNGwcw3qbcOMiIsKZFnfy9L/861h5wKIiMdFxF9ExHcjYjIivhIRzy6Qa8sM2ywjYsOQc3Ui4l2957IHIuLWiHhnRJR4sR8RKyLiooi4vZfvixFx8rBztXXQNmbhr9o4Avgn4C1DzjHoNGADcArNm5w7wGURccRQUzW2Ar8GnNT7+Xvg0xFx4lBT9en9Z3wD8NVhZ+lzPc3bL6Z/hv4BkxFxFPAFYAp4EfA04JeB+4YYa9rJ7Lm9pt/8/4mhJWq8HXgTzXPGU4FfBX4F+MVhhurzAZpt9Vqav7HLgMsj4nFDTdXSQTuVPCK+DFyXmWf3TbsR+FRmnnNQQuxHRCTw8sz81LCzDIqIRwP3AKdl5uZh5xkUEfcCv5KZf1ogy5HAdcCbgfOAr/S+2mWYmdYDL8vMNcPMMSgi3gP8eGYOfS/G/vRGwD8JHJ9DfA9MRHwW+HZm/nzftE8Ck5n52mHl6uV4JM1n6b00My/pm/4V4LOZed6wsrV1UEZOfV+1cdnATbN+1Yb2sqp3ee9QUwyIiJGIeDXNCPSqYefp2QBckpmXDzvIgON7u49vi4iPRsRxww4EvAS4JiI+0dt9/I8R8fphhxrUew75GeCDwyymns8DZ0TEkwEi4oeBU4H/O9RUjQ7NN0c8ODD9AZqMh4yD9UUf8/mqDfVE8ymOFwKfz8yvDzsPQEQ8g6aMDgf+lWbEecNwU0GvKJ9Fs0uoki8DPwt8AziaZkT3xYg4MTO/O8Rcx9F8UeiFwLuBHwHeHxE7MvPPhphr0MuARwEfHmqKxm/TvFj854jYSfPcdm5m/uVwY0Fmbo+Iq4B39PZMfRt4DfCjwL8MNVxLB/tbqNp81YZ2+wPg31Prlc9NwBqaJ4xXAhdHxGnDLKiI+AHgfcDzM3PwleNQZealfVe/1nsCuQX4Ofb8SpmD7TDgmsz89d71f+wdOzwbqFROPw9cmpn7/JqFg+RVNKO4MZrjiGuAiyLirsy8eJjBel4LfBC4k+ZbJK4DxmletB0yDlY5tf6qDTUi4vdpdr2szcytw84zrfcJ9Tf3rl7TOwHhl4A3Di8Vz6b5m7q27ysDRoC1EfEWYHlm7hxWuH6ZeX9EfA04fshR7gYGX1DcSPOCo4SI+EHgecArhp2l573AezLzo73rX+tlPAcYejll5i3Aab2Tp1Zm5t0R8THgtiFHa+WgHHPyqzbai8Yf0PyH/A+ZWf0PK4DlQ85wBc3ZSWv6fq4BPgKsqVJMABGxnOZMr7uHHOULwOBbFJ4MzPWDnA+Gs2hOBrpkfzMeJKPAroFpOyn2vtHMvL9XTEcBLwA+PexMbRzM3Xolv2qjd2bXk/omPSEi1gD3ZuYdw0kFNAf1x4CXAtsjYnrUuS0zHxheLIiIdwOXAt+k+UbLVwPrgBcOMRaZuZ3mU/IfFhH3A98d9rG6iPhd4DPAHTSju/OAlQz/lfbv0Rz7+nXg4zTHnN7Q+xm63nuHzgIuzszusPP0fAY4NyLuoNmt90zgbTS70oYuIl5A82LxJprntvf2/v2hYeZqLTMP2g/Nqb1baD7G/VqaXVUHNcMMmdbRHPca/PnwkHPNlCmB1xXYZn/a9zjeA1wOnDnsXLNk3QRcVCDHR2m+lvohmmMBnwSeNuxcvWw/CXyN5gyvG4HXDztTX7bn9/7unzzsLH2ZVtC8Z/N2mrPgbgHeBTxi2Nl6+f5TL9MOmpH5HwCrhp2r7Y9fmSFJKqfUPlJJksBykiQVZDlJksqxnCRJ5VhOkqRyLCdJUjmWkySpHMtJklSO5SRJKsdykiSVYzlJksqxnCRJ5fx/U9a3KlVvmAwAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-21T09:50:30.364184Z",
     "end_time": "2023-07-21T09:50:30.452566Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
